{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c28ef3e",
   "metadata": {},
   "source": [
    "# 🧭 정적 웹 크롤링 실습\n",
    "전시 산업 참가기업 정보를 `requests`와 `BeautifulSoup`을 사용해 수집하고, 출력 및 CSV로 저장해보는 실습입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0641db3",
   "metadata": {},
   "source": [
    "## 📦 필요한 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4060371d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/nunssuby/anaconda3/envs/crawling/lib/python3.13/site-packages (2.32.3)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.13.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: pandas in /Users/nunssuby/anaconda3/envs/crawling/lib/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/nunssuby/anaconda3/envs/crawling/lib/python3.13/site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nunssuby/anaconda3/envs/crawling/lib/python3.13/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nunssuby/anaconda3/envs/crawling/lib/python3.13/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nunssuby/anaconda3/envs/crawling/lib/python3.13/site-packages (from requests) (2025.1.31)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Using cached soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/nunssuby/anaconda3/envs/crawling/lib/python3.13/site-packages (from beautifulsoup4) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/nunssuby/anaconda3/envs/crawling/lib/python3.13/site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/nunssuby/anaconda3/envs/crawling/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/nunssuby/anaconda3/envs/crawling/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/nunssuby/anaconda3/envs/crawling/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/nunssuby/anaconda3/envs/crawling/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading beautifulsoup4-4.13.3-py3-none-any.whl (186 kB)\n",
      "Using cached soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.13.3 soupsieve-2.6\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5497652",
   "metadata": {},
   "source": [
    "## 🔗 1단계: 웹페이지 요청하기\n",
    "`requests` 라이브러리를 사용해 HTML을 가져옵니다.\n",
    "\n",
    "#### 📌 알아두기: `requests.get()`이 하는 일\n",
    "- 웹사이트에 **\"나 이 페이지 좀 보여줘!\"** 라는 요청을 보냅니다.\n",
    "- 우리가 흔히 브라우저에서 주소창에 입력하는 행동과 같은 일이죠!\n",
    "- 받은 응답은 `response` 변수에 저장되고, 그 안에는 HTML 코드가 담겨 있어요.\n",
    "\n",
    "#### ❗ 왜 `headers`를 써요?\n",
    "- 일부 사이트는 크롤러가 접근하면 막는 경우가 있어요.\n",
    "- `\"User-Agent\"`를 넣으면 **\"사람이 브라우저로 보는 것처럼\"** 속여서 막히지 않게 해주는 역할이에요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "610e31cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "응답 코드: 200\n",
      "HTML 일부: \n",
      "\n",
      "\n",
      "<!DOCTYPE html>\n",
      "<!--[if lt IE 7]>      <html class=\"no-js lt-ie9 lt-ie8 lt-ie7\"> <![endif]-->\n",
      "<!--[if IE 7]>         <html class=\"no-js lt-ie9 lt-ie8\"> <![endif]-->\n",
      "<!--[if IE 8]>         <html class=\"no-js lt-ie9\"> <![endif]-->\n",
      "<!--[if gt IE 8]><!-->\n",
      "<html class=\"no-js\" lang=\"en\">\n",
      "<!--<![endif]-->\n",
      "<head>\n",
      "<link href=\"https://mc-e5b0d581-4409-4340-bc8b-9266-cdn-endpoint.azureedge.net/-/media/feature/experience-accelerator/bootstrap-4/bootstrap-4/styles/optimized-min.css?rev=fb896899\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://www.interzoo.com/en/exhibitor-list\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\"\n",
    "}\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# 응답 확인\n",
    "print(\"응답 코드:\", response.status_code)\n",
    "print(\"HTML 일부:\", response.text[:500])  # 일부만 출력\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee8fe19",
   "metadata": {},
   "source": [
    "## 🧱 2단계: HTML 구조 분석 및 파싱\n",
    "`BeautifulSoup`을 사용해 HTML을 구조화된 형태로 파싱합니다.\n",
    "\n",
    "> `response.text`는 단순한 HTML 텍스트입니다. 구조는 있지만, 우리가 직접 `<div>`나 `<h3>` 같은 태그를 추출해서 사용하기엔 불편하죠.\n",
    "\n",
    "> 그래서 `BeautifulSoup`을 사용해 `soup` 객체로 변환하면 HTML을 **나무 구조(tree)**처럼 다룰 수 있게 되고, 원하는 요소를 `soup.select()`나 `soup.find()`로 쉽게 꺼낼 수 있게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e43781b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<!--[if lt IE 7]>      <html class=\"no-js lt-ie9 lt-ie8 lt-ie7\"> <![endif]-->\n",
      "<!--[if IE 7]>         <html class=\"no-js lt-ie9 lt-ie8\"> <![endif]-->\n",
      "<!--[if IE 8]>         <html class=\"no-js lt-ie9\"> <![endif]-->\n",
      "<!--[if gt IE 8]><!-->\n",
      "<html class=\"no-js\" lang=\"en\">\n",
      " <!--<![endif]-->\n",
      " <head>\n",
      "  <link href=\"https://mc-e5b0d581-4409-4340-bc8b-9266-cdn-endpoint.azureedge.net/-/media/feature/experience-accelerator/bootstrap-4/bootstrap-4/styles/optimized-min.css?rev=fb896899ea424d2192\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# HTML 구조 일부 보기\n",
    "print(soup.prettify()[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2394af",
   "metadata": {},
   "source": [
    "## 🏢 3단계: 기업 정보 리스트 추출하기\n",
    "HTML을 분석하여 기업명, 부스정보, 배지 등을 추출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d96103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exhibitors = soup.select(\"a[href*='/exhibitors/']\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for card in exhibitors:\n",
    "    name = card.select_one(\"h3\")\n",
    "    booth = card.select_one(\"span[aria-label$='Stand.']\")\n",
    "    badges = [badge.text.strip() for badge in card.select(\"div.badges span\")]\n",
    "    link = card['href']\n",
    "\n",
    "    results.append({\n",
    "        \"name\": name.text.strip() if name else \"N/A\",\n",
    "        \"booth\": booth.text.strip() if booth else \"N/A\",\n",
    "        \"badges\": \", \".join(badges),\n",
    "        \"link\": \"https://www.interzoo.com\" + link\n",
    "    })\n",
    "\n",
    "# 일부 출력\n",
    "for r in results[:5]:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe37d45",
   "metadata": {},
   "source": [
    "## 💾 4단계: 결과를 CSV로 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918fd175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"interzoo_exhibitors.csv\", index=False)\n",
    "print(\"CSV 저장 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1790d66",
   "metadata": {},
   "source": [
    "## 🚀 확장 학습 팁\n",
    "- `requests`로 페이징 처리 응용하기\n",
    "- `bs4`로 이미지, 링크 등 추가 수집\n",
    "- Pandas로 정리 후 Excel 저장까지 확장"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
